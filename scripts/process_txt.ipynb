{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2e96bec4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” Äang phÃ¢n tÃ­ch Má»¥c Lá»¥c...\n",
      "âœ… ÄÃ£ tÃ¬m tháº¥y: 64 Tá»‰nh vÃ  1536 Äá»‹a danh chi tiáº¿t.\n",
      "ğŸ“– Äang láº¥y pháº§n Äáº¡i CÆ°Æ¡ng (Äáº§u sÃ¡ch)...\n",
      "ğŸš€ Äang xá»­ lÃ½ chi tiáº¿t tá»«ng tá»‰nh...\n",
      "  -> HÃ€ Ná»˜I: 178 má»¥c.\n",
      "  -> AN GIANG: 17 má»¥c.\n",
      "  -> BÃ€ Rá»ŠA-VÅ¨NG TÃ€U: 33 má»¥c.\n",
      "  -> Báº C LIÃŠU: 12 má»¥c.\n",
      "  -> Báº®C GIANG: 26 má»¥c.\n",
      "  -> Báº®C Káº N: 22 má»¥c.\n",
      "  -> Báº®C NINH: 19 má»¥c.\n",
      "  -> Báº¾N TRE: 19 má»¥c.\n",
      "  -> BÃŒNH DÆ¯Æ NG: 15 má»¥c.\n",
      "  -> BÃŒNH Äá»ŠNH: 24 má»¥c.\n",
      "  -> BÃŒNH PHÆ¯á»šC: 13 má»¥c.\n",
      "  -> BÃŒNH THUáº¬N: 32 má»¥c.\n",
      "  -> CÃ€ MAU: 16 má»¥c.\n",
      "  -> CAO Báº°NG: 30 má»¥c.\n",
      "  -> THÃ€NH PHá» Cáº¦N THÆ : 14 má»¥c.\n",
      "  -> THÃ€NH PHá» ÄÃ€ Náº´NG: 21 má»¥c.\n",
      "  -> Äáº®K Láº®K: 14 má»¥c.\n",
      "  -> Äáº®K NÃ”NG: 16 má»¥c.\n",
      "  -> ÄIá»†N BIÃŠN: 11 má»¥c.\n",
      "  -> Äá»’NG NAI: 18 má»¥c.\n",
      "  -> Äá»’NG THÃP: 18 má»¥c.\n",
      "  -> GIA LAI: 18 má»¥c.\n",
      "  -> HÃ€ GIANG: 25 má»¥c.\n",
      "  -> HÃ€ NAM: 54 má»¥c.\n",
      "  -> HÃ€ TÃ‚Y: 25 má»¥c.\n",
      "  -> HÃ€ TÄ¨NH: 23 má»¥c.\n",
      "  -> Háº¢I DÆ¯Æ NG: 22 má»¥c.\n",
      "  -> Háº¢I PHÃ’NG: 19 má»¥c.\n",
      "  -> Háº¬U GIANG: 7 má»¥c.\n",
      "  -> HÃ’A BÃŒNH: 15 má»¥c.\n",
      "  -> THÃ€NH PHá» Há»’ CHÃ MINH: 48 má»¥c.\n",
      "  -> HÆ¯NG YÃŠN: 38 má»¥c.\n",
      "  -> KIÃŠN GIANG: 27 má»¥c.\n",
      "  -> KON TUM: 13 má»¥c.\n",
      "  -> LAI CHÃ‚U: 6 má»¥c.\n",
      "  -> Láº NG SÆ N: 29 má»¥c.\n",
      "  -> LÃ€O CAI: 23 má»¥c.\n",
      "  -> LÃ‚M Äá»’NG: 37 má»¥c.\n",
      "  -> LONG AN: 13 má»¥c.\n",
      "  -> NAM Äá»ŠNH: 22 má»¥c.\n",
      "  -> NGHá»† AN: 23 má»¥c.\n",
      "  -> NINH BÃŒNH: 21 má»¥c.\n",
      "  -> NINH THUáº¬N: 15 má»¥c.\n",
      "  -> PHÃš THá»Œ: 15 má»¥c.\n",
      "  -> PHÃš YÃŠN: 14 má»¥c.\n",
      "  -> QUáº¢NG BÃŒNH: 20 má»¥c.\n",
      "  -> QUáº¢NG NAM: 28 má»¥c.\n",
      "  -> QUáº¢NG NGÃƒI: 21 má»¥c.\n",
      "  -> QUáº¢NG NINH: 32 má»¥c.\n",
      "  -> QUáº¢NG TRá»Š: 16 má»¥c.\n",
      "  -> SÃ“C TRÄ‚NG: 17 má»¥c.\n",
      "  -> SÆ N LA: 13 má»¥c.\n",
      "  -> TÃ‚Y NINH: 10 má»¥c.\n",
      "  -> THÃI BÃŒNH: 21 má»¥c.\n",
      "  -> THÃI NGUYÃŠN: 17 má»¥c.\n",
      "  -> THANH HÃ“A: 34 má»¥c.\n",
      "  -> THá»ªA THIÃŠN-HUáº¾: 28 má»¥c.\n",
      "  -> TIá»€N GIANG: 21 má»¥c.\n",
      "  -> TRÃ€ VINH: 14 má»¥c.\n",
      "  -> TUYÃŠN QUANG: 18 má»¥c.\n",
      "  -> VÄ¨NH LONG: 14 má»¥c.\n",
      "  -> VÄ¨NH PHÃšC: 14 má»¥c.\n",
      "  -> YÃŠN BÃI: 32 má»¥c.\n",
      "\n",
      "ğŸ‰ HOÃ€N Táº¤T! ÄÃ£ xuáº¥t toÃ n bá»™ dá»¯ liá»‡u ra '../data/VanHoaVaDuLichVN.json'\n",
      "ğŸ“Š Tá»•ng cá»™ng: 63 Tá»‰nh vÃ  1496 Node dá»¯ liá»‡u.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import json\n",
    "import os\n",
    "\n",
    "INPUT_FILENAME = '../data/VanHoaVaDuLichVN.txt'\n",
    "OUTPUT_FILENAME = '../data/VanHoaVaDuLichVN.json'\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"LÃ m sáº¡ch vÄƒn báº£n OCR cÆ¡ báº£n\"\"\"\n",
    "    if not text: return \"\"\n",
    "    # XÃ³a dÃ²ng chá»‰ chá»©a sá»‘ trang\n",
    "    text = re.sub(r'\\n\\s*\\d+\\s*\\n', '\\n', text)\n",
    "    # XÃ³a dáº¥u cháº¥m dáº«n má»¥c lá»¥c\n",
    "    text = re.sub(r'\\.{4,}\\d*', '', text)\n",
    "    # Ná»‘i dÃ²ng bá»‹ ngáº¯t giá»¯a chá»«ng (giá»¯ láº¡i Ä‘oáº¡n vÄƒn)\n",
    "    lines = text.split('\\n')\n",
    "    cleaned_lines = []\n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "        if not line: continue\n",
    "        # Logic ná»‘i dÃ²ng: DÃ²ng trÆ°á»›c khÃ´ng káº¿t thÃºc báº±ng dáº¥u cÃ¢u vÃ  dÃ²ng nÃ y khÃ´ng viáº¿t hoa\n",
    "        if cleaned_lines and not cleaned_lines[-1].endswith(('.', ':', '!', '?')) and not line[0].isupper():\n",
    "            cleaned_lines[-1] += \" \" + line\n",
    "        else:\n",
    "            cleaned_lines.append(line)\n",
    "    return \"\\n\".join(cleaned_lines)\n",
    "\n",
    "def parse_toc_structure(content):\n",
    "    \"\"\"\n",
    "    PhÃ¢n tÃ­ch Má»¥c Lá»¥c Ä‘á»ƒ láº¥y danh sÃ¡ch Tá»‰nh vÃ  Äá»‹a danh chi tiáº¿t\n",
    "    \"\"\"\n",
    "    print(\"ğŸ” Äang phÃ¢n tÃ­ch Má»¥c Lá»¥c...\")\n",
    "    \n",
    "    # 1. TÃ¬m vÃ¹ng Má»¥c lá»¥c\n",
    "    start_marker = \"Báº¢NG TRA NHANH CÃC Tá»ˆNH\"\n",
    "    start_idx = content.rfind(start_marker)\n",
    "    if start_idx == -1: \n",
    "        # Fallback\n",
    "        start_idx = content.rfind(\"Báº¢NG TRA NHANH\")\n",
    "    \n",
    "    if start_idx == -1:\n",
    "        print(\"âŒ KhÃ´ng tÃ¬m tháº¥y má»¥c lá»¥c báº£ng tra. Kiá»ƒm tra láº¡i file.\")\n",
    "        return [], []\n",
    "\n",
    "    toc_content = content[start_idx:]\n",
    "    \n",
    "    # 2. TÃ¡ch pháº§n Tá»‰nh vÃ  pháº§n Di tÃ­ch\n",
    "    # Dá»±a vÃ o tá»« khÃ³a ngÄƒn cÃ¡ch trong sÃ¡ch\n",
    "    split_marker = \"Báº¢NG TRA CÃC THáº®NG Cáº¢NH\"\n",
    "    parts = toc_content.split(split_marker)\n",
    "    \n",
    "    toc_provinces_text = parts[0]\n",
    "    toc_attractions_text = parts[1] if len(parts) > 1 else \"\"\n",
    "\n",
    "    # Regex báº¯t dÃ²ng: TÃªn ... Sá»‘ trang\n",
    "    line_pattern = re.compile(r'^(.+?)(?:\\.{3,})\\s*\\d+$', re.MULTILINE)\n",
    "\n",
    "    # Danh sÃ¡ch Tá»‰nh\n",
    "    provinces = []\n",
    "    for m in line_pattern.findall(toc_provinces_text):\n",
    "        name = m.strip()\n",
    "        if len(name) > 3 and \"Báº¢NG TRA\" not in name:\n",
    "            provinces.append(name)\n",
    "\n",
    "    # Danh sÃ¡ch Chi tiáº¿t (Tháº¯ng cáº£nh/Di tÃ­ch)\n",
    "    attractions = []\n",
    "    for m in line_pattern.findall(toc_attractions_text):\n",
    "        name = m.strip()\n",
    "        if len(name) > 3:\n",
    "            attractions.append(name)\n",
    "\n",
    "    # Sáº¯p xáº¿p theo Ä‘á»™ dÃ i giáº£m dáº§n Ä‘á»ƒ regex Æ°u tiÃªn chuá»—i dÃ i\n",
    "    provinces = sorted(list(set(provinces)), key=len, reverse=True)\n",
    "    attractions = sorted(list(set(attractions)), key=len, reverse=True)\n",
    "    \n",
    "    print(f\"âœ… ÄÃ£ tÃ¬m tháº¥y: {len(provinces)} Tá»‰nh vÃ  {len(attractions)} Äá»‹a danh chi tiáº¿t.\")\n",
    "    return provinces, attractions\n",
    "\n",
    "def extract_general_info(content, first_province):\n",
    "    \"\"\"Láº¥y pháº§n Ä‘áº§u sÃ¡ch (trÆ°á»›c khi vÃ o tá»‰nh Ä‘áº§u tiÃªn)\"\"\"\n",
    "    print(\"ğŸ“– Äang láº¥y pháº§n Äáº¡i CÆ°Æ¡ng (Äáº§u sÃ¡ch)...\")\n",
    "    # TÃ¬m vá»‹ trÃ­ tá»‰nh Ä‘áº§u tiÃªn (thÆ°á»ng lÃ  AN GIANG)\n",
    "    match = re.search(rf'\\n\\s*{re.escape(first_province)}\\s*\\n', content)\n",
    "    if not match:\n",
    "        match = re.search(rf'{re.escape(first_province)}', content)\n",
    "    \n",
    "    end_idx = match.start() if match else 0\n",
    "    general_text = content[:end_idx]\n",
    "    \n",
    "    # Chia nhá» pháº§n Ä‘áº¡i cÆ°Æ¡ng theo cÃ¡c chÆ°Æ¡ng lá»›n (Hardcode cÃ¡c chÆ°Æ¡ng phá»• biáº¿n)\n",
    "    chapters = [\n",
    "        \"LÃƒNH THá»” VIá»†T NAM\", \"Lá»ŠCH Sá»¬ VIá»†T NAM\", \"CÃC DÃ‚N Tá»˜C TRÃŠN Äáº¤T NÆ¯á»šC VIá»†T NAM\",\n",
    "        \"TÃ”N GIÃO - TÃN NGÆ¯á» NG\", \"PHONG Tá»¤C Táº¬P QUÃN\", \"Lá»„ Há»˜I TRUYá»€N THá»NG\", \"Ä‚N Máº¶C Cá»¦A NGÆ¯á»œI VIá»†T\"\n",
    "    ]\n",
    "    \n",
    "    data = []\n",
    "    # Cáº¯t text theo chÆ°Æ¡ng\n",
    "    indices = []\n",
    "    for chap in chapters:\n",
    "        m = re.search(rf'(?:\\n|^)\\s*{re.escape(chap)}', general_text)\n",
    "        if m: indices.append({\"name\": chap, \"start\": m.start()})\n",
    "    \n",
    "    indices.sort(key=lambda x: x[\"start\"])\n",
    "    \n",
    "    for i, item in enumerate(indices):\n",
    "        name = item[\"name\"]\n",
    "        start = item[\"start\"]\n",
    "        end = indices[i+1][\"start\"] if i+1 < len(indices) else len(general_text)\n",
    "        \n",
    "        content_chap = clean_text(general_text[start:end])\n",
    "        data.append({\n",
    "            \"id\": f\"GEN_{i:02d}\",\n",
    "            \"name\": name,\n",
    "            \"type\": \"GeneralTopic\",\n",
    "            \"content\": content_chap,\n",
    "            \"embedding_text\": f\"Tá»•ng quan Viá»‡t Nam - {name}: {content_chap[:500]}\"\n",
    "        })\n",
    "        \n",
    "    return data\n",
    "\n",
    "def extract_all_content(file_path):\n",
    "    if not os.path.exists(file_path):\n",
    "        print(f\"âŒ File not found: {file_path}\")\n",
    "        return\n",
    "\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        content = f.read()\n",
    "\n",
    "    # 1. Parse Má»¥c Lá»¥c\n",
    "    provinces, attractions = parse_toc_structure(content)\n",
    "    if not provinces: return\n",
    "\n",
    "    final_dataset = {\n",
    "        \"General_Information\": extract_general_info(content, provinces[0]), # Láº¥y pháº§n Ä‘áº§u sÃ¡ch\n",
    "        \"Provinces\": []\n",
    "    }\n",
    "\n",
    "    # 2. Cáº¯t ná»™i dung theo tá»«ng Tá»‰nh\n",
    "    print(\"ğŸš€ Äang xá»­ lÃ½ chi tiáº¿t tá»«ng tá»‰nh...\")\n",
    "    \n",
    "    # TÃ¬m vá»‹ trÃ­ báº¯t Ä‘áº§u cá»§a má»—i tá»‰nh trong text\n",
    "    prov_markers = []\n",
    "    for p in provinces:\n",
    "        # Regex tÃ¬m tÃªn tá»‰nh Ä‘á»©ng riÃªng dÃ²ng hoáº·c in hoa\n",
    "        m = re.search(rf'(?:\\n|^)\\s*{re.escape(p)}\\s*(?:\\n|$)', content)\n",
    "        if m: prov_markers.append({\"name\": p, \"start\": m.start()})\n",
    "    \n",
    "    prov_markers.sort(key=lambda x: x[\"start\"])\n",
    "\n",
    "    for i, p_mk in enumerate(prov_markers):\n",
    "        p_name = p_mk[\"name\"]\n",
    "        start = p_mk[\"start\"]\n",
    "        # Káº¿t thÃºc lÃ  tá»‰nh tiáº¿p theo hoáº·c Ä‘áº¿n pháº§n Má»¥c Lá»¥c\n",
    "        end = prov_markers[i+1][\"start\"] if i+1 < len(prov_markers) else content.rfind(\"Báº¢NG TRA NHANH\")\n",
    "        \n",
    "        prov_text = content[start:end]\n",
    "        \n",
    "        province_obj = {\n",
    "            \"id\": f\"P_{i:03d}\",\n",
    "            \"name\": p_name,\n",
    "            \"items\": []\n",
    "        }\n",
    "\n",
    "        # 3. Trong má»—i tá»‰nh, quÃ©t tÃ¬m cÃ¡c Äá»‹a danh/Lá»… há»™i tá»« danh sÃ¡ch Attractions\n",
    "        found_attrs = []\n",
    "        for attr in attractions:\n",
    "            # TÃ¬m Ä‘á»‹a danh trong text cá»§a tá»‰nh nÃ y\n",
    "            # Cho phÃ©p sá»‘ thá»© tá»±: \"1. ChÃ¹a HÆ°Æ¡ng\"\n",
    "            am = re.search(rf'(?:\\n|^)\\s*(?:\\d+\\.\\s*)?{re.escape(attr)}\\s*(?:\\n|\\.\\.\\.)', prov_text, re.IGNORECASE)\n",
    "            if am:\n",
    "                found_attrs.append({\"name\": attr, \"start\": am.start(), \"end_header\": am.end()})\n",
    "        \n",
    "        found_attrs.sort(key=lambda x: x[\"start\"])\n",
    "        \n",
    "        # 4. Cáº¯t text cho tá»«ng Ä‘á»‹a danh tÃ¬m tháº¥y\n",
    "        if not found_attrs:\n",
    "            # Náº¿u khÃ´ng tÃ¬m tháº¥y Ä‘á»‹a danh nÃ o, láº¥y toÃ n bá»™ text lÃ m \"Giá»›i thiá»‡u chung\"\n",
    "            province_obj[\"items\"].append({\n",
    "                \"name\": f\"Tá»•ng quan {p_name}\",\n",
    "                \"type\": \"General\",\n",
    "                \"content\": clean_text(prov_text.replace(p_name, \"\"))\n",
    "            })\n",
    "        else:\n",
    "            # Láº¥y pháº§n Ä‘áº§u (Intro)\n",
    "            intro_text = prov_text[:found_attrs[0][\"start\"]].replace(p_name, \"\")\n",
    "            if len(intro_text.strip()) > 50:\n",
    "                province_obj[\"items\"].append({\n",
    "                    \"name\": f\"Giá»›i thiá»‡u chung {p_name}\",\n",
    "                    \"type\": \"General\",\n",
    "                    \"content\": clean_text(intro_text),\n",
    "                    \"embedding_text\": f\"Tá»•ng quan vá» tá»‰nh {p_name}. {intro_text[:300]}\"\n",
    "                })\n",
    "            \n",
    "            # Láº¥y tá»«ng Ä‘á»‹a danh\n",
    "            for k, attr in enumerate(found_attrs):\n",
    "                a_name = attr[\"name\"]\n",
    "                a_start = attr[\"end_header\"]\n",
    "                a_end = found_attrs[k+1][\"start\"] if k+1 < len(found_attrs) else len(prov_text)\n",
    "                \n",
    "                a_content = clean_text(prov_text[a_start:a_end])\n",
    "                \n",
    "                if len(a_content) > 20: # Bá» qua rÃ¡c\n",
    "                    # Auto-Labeling Ä‘Æ¡n giáº£n\n",
    "                    label = \"Place\"\n",
    "                    if any(x in a_name.upper() for x in [\"Lá»„\", \"Há»˜I\"]): label = \"Festival\"\n",
    "                    elif any(x in a_name.upper() for x in [\"CHÃ™A\", \"Äá»€N\", \"ÄÃŒNH\", \"MIáº¾U\"]): label = \"Religious\"\n",
    "                    elif any(x in a_name.upper() for x in [\"NÃšI\", \"SÃ”NG\", \"Há»’\", \"THÃC\", \"Rá»ªNG\"]): label = \"Nature\"\n",
    "                    \n",
    "                    province_obj[\"items\"].append({\n",
    "                        \"name\": a_name,\n",
    "                        \"type\": label,\n",
    "                        \"content\": a_content,\n",
    "                        \"embedding_text\": f\"{a_name} ({label}) táº¡i {p_name}. {a_content[:300]}\"\n",
    "                    })\n",
    "\n",
    "        final_dataset[\"Provinces\"].append(province_obj)\n",
    "        print(f\"  -> {p_name}: {len(province_obj['items'])} má»¥c.\")\n",
    "\n",
    "    # Xuáº¥t file JSON\n",
    "    with open(OUTPUT_FILENAME, 'w', encoding='utf-8') as f:\n",
    "        json.dump(final_dataset, f, ensure_ascii=False, indent=2)\n",
    "    \n",
    "    print(f\"\\nğŸ‰ HOÃ€N Táº¤T! ÄÃ£ xuáº¥t toÃ n bá»™ dá»¯ liá»‡u ra '{OUTPUT_FILENAME}'\")\n",
    "    \n",
    "    # Thá»‘ng kÃª\n",
    "    total_items = sum(len(p['items']) for p in final_dataset['Provinces']) + len(final_dataset['General_Information'])\n",
    "    print(f\"ğŸ“Š Tá»•ng cá»™ng: {len(final_dataset['Provinces'])} Tá»‰nh vÃ  {total_items} Node dá»¯ liá»‡u.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    extract_all_content(INPUT_FILENAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d24819f6",
   "metadata": {},
   "source": [
    "### Lá»—i : 2 váº¥n Ä‘á» lá»›n á»Ÿ pháº§n cuá»‘i file\n",
    "1. Lá»—i \"Nuá»‘t\" Má»¥c Lá»¥c (Critical):\n",
    "\n",
    "Triá»‡u chá»©ng: Trong pháº§n tá»‰nh YÃŠN BÃI, xuáº¥t hiá»‡n cÃ¡c Ä‘á»‹a danh khÃ´ng liÃªn quan nhÆ°: \"MÅ©i Nai\" (thá»±c ra á»Ÿ KiÃªn Giang), \"LÃ ng ngÆ°á»i Ba Na\" (TÃ¢y NguyÃªn).\n",
    "\n",
    "NguyÃªn nhÃ¢n: Script hiá»‡n táº¡i Ä‘ang láº¥y ná»™i dung tá»‰nh YÃªn BÃ¡i kÃ©o dÃ i Ä‘áº¿n háº¿t file. MÃ  cuá»‘i file láº¡i lÃ  pháº§n Má»¥c Lá»¥c (Báº£ng tra nhanh) chá»©a tÃªn cá»§a táº¥t cáº£ Ä‘á»‹a danh. Code Ä‘á»c tháº¥y tÃªn \"MÅ©i Nai\" trong má»¥c lá»¥c náº±m á»Ÿ Ä‘oáº¡n cuá»‘i, nÃ³ tÆ°á»Ÿng nháº§m \"MÅ©i Nai\" thuá»™c vá» YÃªn BÃ¡i.\n",
    "\n",
    "2. Lá»—i Ná»™i dung rá»—ng:\n",
    "\n",
    "Triá»‡u chá»©ng: Má»™t sá»‘ má»¥c cÃ³ content lÃ : \"THáº®NG Cáº¢NH, DI TÃCH VÃ€ Lá»„ Há»˜I\". ÄÃ¢y lÃ  tiÃªu Ä‘á» trang (Header/Footer) chá»© khÃ´ng pháº£i ná»™i dung mÃ´ táº£.\n",
    "\n",
    "NguyÃªn nhÃ¢n: Regex cáº¯t chuá»—i bá»‹ báº¯t nháº§m vÃ o tiÃªu Ä‘á» láº·p láº¡i cá»§a sÃ¡ch."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eb41a12",
   "metadata": {},
   "source": [
    "### Fix lá»—i\n",
    "1. DÃ²ng body_content = content[:toc_idx]: ÄÃ¢y lÃ  chÃ¬a khÃ³a. Ã©p chÆ°Æ¡ng trÃ¬nh cáº¯t Ä‘á»©t Ä‘uÃ´i file táº¡i vá»‹ trÃ­ báº¯t Ä‘áº§u Má»¥c Lá»¥c. Do Ä‘Ã³, khi xá»­ lÃ½ tá»‰nh cuá»‘i cÃ¹ng (YÃªn BÃ¡i), nÃ³ sáº½ dá»«ng láº¡i Ä‘Ãºng lÃºc, khÃ´ng Ä‘á»c trÃ n sang pháº§n Má»¥c lá»¥c chá»©a tÃªn \"MÅ©i Nai\" hay \"LÃ ng ngÆ°á»i Ba Na\".\n",
    "\n",
    "2. DÃ²ng re.sub(r'THáº®NG Cáº¢NH...', '', raw_content): Loáº¡i bá» cÃ¡c dÃ²ng tiÃªu Ä‘á» trang (Header) vÃ´ nghÄ©a bá»‹ dÃ­nh vÃ o ná»™i dung."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c0e801b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” Äang phÃ¢n tÃ­ch Má»¥c Lá»¥c...\n",
      "âœ… ÄÃ£ tÃ¬m tháº¥y: 64 Tá»‰nh vÃ  1536 Äá»‹a danh chi tiáº¿t.\n",
      "ğŸ“– Äang láº¥y pháº§n Äáº¡i CÆ°Æ¡ng (Äáº§u sÃ¡ch)...\n",
      "ğŸš€ Äang xá»­ lÃ½ chi tiáº¿t tá»«ng tá»‰nh (ÄÃ£ Fix lá»—i YÃªn BÃ¡i)...\n",
      "  -> HÃ€ Ná»˜I: 178 má»¥c.\n",
      "  -> AN GIANG: 17 má»¥c.\n",
      "  -> BÃ€ Rá»ŠA-VÅ¨NG TÃ€U: 33 má»¥c.\n",
      "  -> Báº C LIÃŠU: 12 má»¥c.\n",
      "  -> Báº®C GIANG: 26 má»¥c.\n",
      "  -> Báº®C Káº N: 22 má»¥c.\n",
      "  -> Báº®C NINH: 19 má»¥c.\n",
      "  -> Báº¾N TRE: 19 má»¥c.\n",
      "  -> BÃŒNH DÆ¯Æ NG: 15 má»¥c.\n",
      "  -> BÃŒNH Äá»ŠNH: 24 má»¥c.\n",
      "  -> BÃŒNH PHÆ¯á»šC: 13 má»¥c.\n",
      "  -> BÃŒNH THUáº¬N: 32 má»¥c.\n",
      "  -> CÃ€ MAU: 16 má»¥c.\n",
      "  -> CAO Báº°NG: 30 má»¥c.\n",
      "  -> THÃ€NH PHá» Cáº¦N THÆ : 14 má»¥c.\n",
      "  -> THÃ€NH PHá» ÄÃ€ Náº´NG: 21 má»¥c.\n",
      "  -> Äáº®K Láº®K: 14 má»¥c.\n",
      "  -> Äáº®K NÃ”NG: 16 má»¥c.\n",
      "  -> ÄIá»†N BIÃŠN: 11 má»¥c.\n",
      "  -> Äá»’NG NAI: 18 má»¥c.\n",
      "  -> Äá»’NG THÃP: 18 má»¥c.\n",
      "  -> GIA LAI: 18 má»¥c.\n",
      "  -> HÃ€ GIANG: 25 má»¥c.\n",
      "  -> HÃ€ NAM: 54 má»¥c.\n",
      "  -> HÃ€ TÃ‚Y: 25 má»¥c.\n",
      "  -> HÃ€ TÄ¨NH: 23 má»¥c.\n",
      "  -> Háº¢I DÆ¯Æ NG: 22 má»¥c.\n",
      "  -> Háº¢I PHÃ’NG: 19 má»¥c.\n",
      "  -> Háº¬U GIANG: 7 má»¥c.\n",
      "  -> HÃ’A BÃŒNH: 15 má»¥c.\n",
      "  -> THÃ€NH PHá» Há»’ CHÃ MINH: 48 má»¥c.\n",
      "  -> HÆ¯NG YÃŠN: 38 má»¥c.\n",
      "  -> KIÃŠN GIANG: 27 má»¥c.\n",
      "  -> KON TUM: 13 má»¥c.\n",
      "  -> LAI CHÃ‚U: 6 má»¥c.\n",
      "  -> Láº NG SÆ N: 29 má»¥c.\n",
      "  -> LÃ€O CAI: 23 má»¥c.\n",
      "  -> LÃ‚M Äá»’NG: 37 má»¥c.\n",
      "  -> LONG AN: 13 má»¥c.\n",
      "  -> NAM Äá»ŠNH: 22 má»¥c.\n",
      "  -> NGHá»† AN: 23 má»¥c.\n",
      "  -> NINH BÃŒNH: 21 má»¥c.\n",
      "  -> NINH THUáº¬N: 15 má»¥c.\n",
      "  -> PHÃš THá»Œ: 15 má»¥c.\n",
      "  -> PHÃš YÃŠN: 14 má»¥c.\n",
      "  -> QUáº¢NG BÃŒNH: 20 má»¥c.\n",
      "  -> QUáº¢NG NAM: 28 má»¥c.\n",
      "  -> QUáº¢NG NGÃƒI: 21 má»¥c.\n",
      "  -> QUáº¢NG NINH: 32 má»¥c.\n",
      "  -> QUáº¢NG TRá»Š: 16 má»¥c.\n",
      "  -> SÃ“C TRÄ‚NG: 17 má»¥c.\n",
      "  -> SÆ N LA: 13 má»¥c.\n",
      "  -> TÃ‚Y NINH: 10 má»¥c.\n",
      "  -> THÃI BÃŒNH: 21 má»¥c.\n",
      "  -> THÃI NGUYÃŠN: 17 má»¥c.\n",
      "  -> THANH HÃ“A: 34 má»¥c.\n",
      "  -> THá»ªA THIÃŠN-HUáº¾: 28 má»¥c.\n",
      "  -> TIá»€N GIANG: 21 má»¥c.\n",
      "  -> TRÃ€ VINH: 14 má»¥c.\n",
      "  -> TUYÃŠN QUANG: 18 má»¥c.\n",
      "  -> VÄ¨NH LONG: 14 má»¥c.\n",
      "  -> VÄ¨NH PHÃšC: 14 má»¥c.\n",
      "  -> YÃŠN BÃI: 14 má»¥c.\n",
      "\n",
      "âœ… HOÃ€N Táº¤T FIX! File '../data/VanHoaVaDuLichVN.json' Ä‘Ã£ sáº¡ch lá»—i YÃªn BÃ¡i.\n",
      "ğŸ” Äang phÃ¢n tÃ­ch Má»¥c Lá»¥c...\n",
      "âœ… ÄÃ£ tÃ¬m tháº¥y: 64 Tá»‰nh vÃ  1536 Äá»‹a danh chi tiáº¿t.\n",
      "ğŸ“– Äang láº¥y pháº§n Äáº¡i CÆ°Æ¡ng (Äáº§u sÃ¡ch)...\n",
      "ğŸš€ Äang xá»­ lÃ½ chi tiáº¿t tá»«ng tá»‰nh...\n",
      "  -> HÃ€ Ná»˜I: 178 má»¥c.\n",
      "  -> AN GIANG: 17 má»¥c.\n",
      "  -> BÃ€ Rá»ŠA-VÅ¨NG TÃ€U: 33 má»¥c.\n",
      "  -> Báº C LIÃŠU: 12 má»¥c.\n",
      "  -> Báº®C GIANG: 26 má»¥c.\n",
      "  -> Báº®C Káº N: 22 má»¥c.\n",
      "  -> Báº®C NINH: 19 má»¥c.\n",
      "  -> Báº¾N TRE: 19 má»¥c.\n",
      "  -> BÃŒNH DÆ¯Æ NG: 15 má»¥c.\n",
      "  -> BÃŒNH Äá»ŠNH: 24 má»¥c.\n",
      "  -> BÃŒNH PHÆ¯á»šC: 13 má»¥c.\n",
      "  -> BÃŒNH THUáº¬N: 32 má»¥c.\n",
      "  -> CÃ€ MAU: 16 má»¥c.\n",
      "  -> CAO Báº°NG: 30 má»¥c.\n",
      "  -> THÃ€NH PHá» Cáº¦N THÆ : 14 má»¥c.\n",
      "  -> THÃ€NH PHá» ÄÃ€ Náº´NG: 21 má»¥c.\n",
      "  -> Äáº®K Láº®K: 14 má»¥c.\n",
      "  -> Äáº®K NÃ”NG: 16 má»¥c.\n",
      "  -> ÄIá»†N BIÃŠN: 11 má»¥c.\n",
      "  -> Äá»’NG NAI: 18 má»¥c.\n",
      "  -> Äá»’NG THÃP: 18 má»¥c.\n",
      "  -> GIA LAI: 18 má»¥c.\n",
      "  -> HÃ€ GIANG: 25 má»¥c.\n",
      "  -> HÃ€ NAM: 54 má»¥c.\n",
      "  -> HÃ€ TÃ‚Y: 25 má»¥c.\n",
      "  -> HÃ€ TÄ¨NH: 23 má»¥c.\n",
      "  -> Háº¢I DÆ¯Æ NG: 22 má»¥c.\n",
      "  -> Háº¢I PHÃ’NG: 19 má»¥c.\n",
      "  -> Háº¬U GIANG: 7 má»¥c.\n",
      "  -> HÃ’A BÃŒNH: 15 má»¥c.\n",
      "  -> THÃ€NH PHá» Há»’ CHÃ MINH: 48 má»¥c.\n",
      "  -> HÆ¯NG YÃŠN: 38 má»¥c.\n",
      "  -> KIÃŠN GIANG: 27 má»¥c.\n",
      "  -> KON TUM: 13 má»¥c.\n",
      "  -> LAI CHÃ‚U: 6 má»¥c.\n",
      "  -> Láº NG SÆ N: 29 má»¥c.\n",
      "  -> LÃ€O CAI: 23 má»¥c.\n",
      "  -> LÃ‚M Äá»’NG: 37 má»¥c.\n",
      "  -> LONG AN: 13 má»¥c.\n",
      "  -> NAM Äá»ŠNH: 22 má»¥c.\n",
      "  -> NGHá»† AN: 23 má»¥c.\n",
      "  -> NINH BÃŒNH: 21 má»¥c.\n",
      "  -> NINH THUáº¬N: 15 má»¥c.\n",
      "  -> PHÃš THá»Œ: 15 má»¥c.\n",
      "  -> PHÃš YÃŠN: 14 má»¥c.\n",
      "  -> QUáº¢NG BÃŒNH: 20 má»¥c.\n",
      "  -> QUáº¢NG NAM: 28 má»¥c.\n",
      "  -> QUáº¢NG NGÃƒI: 21 má»¥c.\n",
      "  -> QUáº¢NG NINH: 32 má»¥c.\n",
      "  -> QUáº¢NG TRá»Š: 16 má»¥c.\n",
      "  -> SÃ“C TRÄ‚NG: 17 má»¥c.\n",
      "  -> SÆ N LA: 13 má»¥c.\n",
      "  -> TÃ‚Y NINH: 10 má»¥c.\n",
      "  -> THÃI BÃŒNH: 21 má»¥c.\n",
      "  -> THÃI NGUYÃŠN: 17 má»¥c.\n",
      "  -> THANH HÃ“A: 34 má»¥c.\n",
      "  -> THá»ªA THIÃŠN-HUáº¾: 28 má»¥c.\n",
      "  -> TIá»€N GIANG: 21 má»¥c.\n",
      "  -> TRÃ€ VINH: 14 má»¥c.\n",
      "  -> TUYÃŠN QUANG: 18 má»¥c.\n",
      "  -> VÄ¨NH LONG: 14 má»¥c.\n",
      "  -> VÄ¨NH PHÃšC: 14 má»¥c.\n",
      "  -> YÃŠN BÃI: 32 má»¥c.\n",
      "\n",
      "ğŸ‰ HOÃ€N Táº¤T! ÄÃ£ xuáº¥t toÃ n bá»™ dá»¯ liá»‡u ra '../data/VanHoaVaDuLichVN.json'\n",
      "ğŸ“Š Tá»•ng cá»™ng: 63 Tá»‰nh vÃ  1496 Node dá»¯ liá»‡u.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import json\n",
    "import os\n",
    "\n",
    "INPUT_FILENAME = '../data/VanHoaVaDuLichVN.txt'\n",
    "OUTPUT_FILENAME = '../data/VanHoaVaDuLichVN.json'\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"LÃ m sáº¡ch vÄƒn báº£n OCR cÆ¡ báº£n\"\"\"\n",
    "    if not text: return \"\"\n",
    "    # XÃ³a dÃ²ng chá»‰ chá»©a sá»‘ trang\n",
    "    text = re.sub(r'\\n\\s*\\d+\\s*\\n', '\\n', text)\n",
    "    # XÃ³a dáº¥u cháº¥m dáº«n má»¥c lá»¥c\n",
    "    text = re.sub(r'\\.{4,}\\d*', '', text)\n",
    "    # Ná»‘i dÃ²ng bá»‹ ngáº¯t giá»¯a chá»«ng (giá»¯ láº¡i Ä‘oáº¡n vÄƒn)\n",
    "    lines = text.split('\\n')\n",
    "    cleaned_lines = []\n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "        if not line: continue\n",
    "        # Logic ná»‘i dÃ²ng: DÃ²ng trÆ°á»›c khÃ´ng káº¿t thÃºc báº±ng dáº¥u cÃ¢u vÃ  dÃ²ng nÃ y khÃ´ng viáº¿t hoa\n",
    "        if cleaned_lines and not cleaned_lines[-1].endswith(('.', ':', '!', '?')) and not line[0].isupper():\n",
    "            cleaned_lines[-1] += \" \" + line\n",
    "        else:\n",
    "            cleaned_lines.append(line)\n",
    "    return \"\\n\".join(cleaned_lines)\n",
    "\n",
    "def parse_toc_structure(content):\n",
    "    \"\"\"\n",
    "    PhÃ¢n tÃ­ch Má»¥c Lá»¥c Ä‘á»ƒ láº¥y danh sÃ¡ch Tá»‰nh vÃ  Äá»‹a danh chi tiáº¿t\n",
    "    \"\"\"\n",
    "    print(\"ğŸ” Äang phÃ¢n tÃ­ch Má»¥c Lá»¥c...\")\n",
    "    \n",
    "    # 1. TÃ¬m vÃ¹ng Má»¥c lá»¥c\n",
    "    start_marker = \"Báº¢NG TRA NHANH CÃC Tá»ˆNH\"\n",
    "    start_idx = content.rfind(start_marker)\n",
    "    if start_idx == -1: \n",
    "        # Fallback\n",
    "        start_idx = content.rfind(\"Báº¢NG TRA NHANH\")\n",
    "    \n",
    "    if start_idx == -1:\n",
    "        print(\"âŒ KhÃ´ng tÃ¬m tháº¥y má»¥c lá»¥c báº£ng tra. Kiá»ƒm tra láº¡i file.\")\n",
    "        return [], []\n",
    "\n",
    "    toc_content = content[start_idx:]\n",
    "    \n",
    "    # 2. TÃ¡ch pháº§n Tá»‰nh vÃ  pháº§n Di tÃ­ch\n",
    "    # Dá»±a vÃ o tá»« khÃ³a ngÄƒn cÃ¡ch trong sÃ¡ch\n",
    "    split_marker = \"Báº¢NG TRA CÃC THáº®NG Cáº¢NH\"\n",
    "    parts = toc_content.split(split_marker)\n",
    "    \n",
    "    toc_provinces_text = parts[0]\n",
    "    toc_attractions_text = parts[1] if len(parts) > 1 else \"\"\n",
    "\n",
    "    # Regex báº¯t dÃ²ng: TÃªn ... Sá»‘ trang\n",
    "    line_pattern = re.compile(r'^(.+?)(?:\\.{3,})\\s*\\d+$', re.MULTILINE)\n",
    "\n",
    "    # Danh sÃ¡ch Tá»‰nh\n",
    "    provinces = []\n",
    "    for m in line_pattern.findall(toc_provinces_text):\n",
    "        name = m.strip()\n",
    "        if len(name) > 3 and \"Báº¢NG TRA\" not in name:\n",
    "            provinces.append(name)\n",
    "\n",
    "    # Danh sÃ¡ch Chi tiáº¿t (Tháº¯ng cáº£nh/Di tÃ­ch)\n",
    "    attractions = []\n",
    "    for m in line_pattern.findall(toc_attractions_text):\n",
    "        name = m.strip()\n",
    "        if len(name) > 3:\n",
    "            attractions.append(name)\n",
    "\n",
    "    # Sáº¯p xáº¿p theo Ä‘á»™ dÃ i giáº£m dáº§n Ä‘á»ƒ regex Æ°u tiÃªn chuá»—i dÃ i\n",
    "    provinces = sorted(list(set(provinces)), key=len, reverse=True)\n",
    "    attractions = sorted(list(set(attractions)), key=len, reverse=True)\n",
    "    \n",
    "    print(f\"âœ… ÄÃ£ tÃ¬m tháº¥y: {len(provinces)} Tá»‰nh vÃ  {len(attractions)} Äá»‹a danh chi tiáº¿t.\")\n",
    "    return provinces, attractions\n",
    "\n",
    "def extract_general_info(content, first_province):\n",
    "    \"\"\"Láº¥y pháº§n Ä‘áº§u sÃ¡ch (trÆ°á»›c khi vÃ o tá»‰nh Ä‘áº§u tiÃªn)\"\"\"\n",
    "    print(\"ğŸ“– Äang láº¥y pháº§n Äáº¡i CÆ°Æ¡ng (Äáº§u sÃ¡ch)...\")\n",
    "    # TÃ¬m vá»‹ trÃ­ tá»‰nh Ä‘áº§u tiÃªn (thÆ°á»ng lÃ  AN GIANG)\n",
    "    match = re.search(rf'\\n\\s*{re.escape(first_province)}\\s*\\n', content)\n",
    "    if not match:\n",
    "        match = re.search(rf'{re.escape(first_province)}', content)\n",
    "    \n",
    "    end_idx = match.start() if match else 0\n",
    "    general_text = content[:end_idx]\n",
    "    \n",
    "    # Chia nhá» pháº§n Ä‘áº¡i cÆ°Æ¡ng theo cÃ¡c chÆ°Æ¡ng lá»›n (Hardcode cÃ¡c chÆ°Æ¡ng phá»• biáº¿n)\n",
    "    chapters = [\n",
    "        \"LÃƒNH THá»” VIá»†T NAM\", \"Lá»ŠCH Sá»¬ VIá»†T NAM\", \"CÃC DÃ‚N Tá»˜C TRÃŠN Äáº¤T NÆ¯á»šC VIá»†T NAM\",\n",
    "        \"TÃ”N GIÃO - TÃN NGÆ¯á» NG\", \"PHONG Tá»¤C Táº¬P QUÃN\", \"Lá»„ Há»˜I TRUYá»€N THá»NG\", \"Ä‚N Máº¶C Cá»¦A NGÆ¯á»œI VIá»†T\"\n",
    "    ]\n",
    "    \n",
    "    data = []\n",
    "    # Cáº¯t text theo chÆ°Æ¡ng\n",
    "    indices = []\n",
    "    for chap in chapters:\n",
    "        m = re.search(rf'(?:\\n|^)\\s*{re.escape(chap)}', general_text)\n",
    "        if m: indices.append({\"name\": chap, \"start\": m.start()})\n",
    "    \n",
    "    indices.sort(key=lambda x: x[\"start\"])\n",
    "    \n",
    "    for i, item in enumerate(indices):\n",
    "        name = item[\"name\"]\n",
    "        start = item[\"start\"]\n",
    "        end = indices[i+1][\"start\"] if i+1 < len(indices) else len(general_text)\n",
    "        \n",
    "        content_chap = clean_text(general_text[start:end])\n",
    "        data.append({\n",
    "            \"id\": f\"GEN_{i:02d}\",\n",
    "            \"name\": name,\n",
    "            \"type\": \"GeneralTopic\",\n",
    "            \"content\": content_chap,\n",
    "            \"embedding_text\": f\"Tá»•ng quan Viá»‡t Nam - {name}: {content_chap[:500]}\"\n",
    "        })\n",
    "        \n",
    "    return data\n",
    "\n",
    "def extract_all_content(file_path):\n",
    "    if not os.path.exists(file_path):\n",
    "        print(f\"âŒ File not found: {file_path}\")\n",
    "        return\n",
    "\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        content = f.read()\n",
    "\n",
    "    # --- FIX 1: Cáº®T Bá» HOÃ€N TOÃ€N PHáº¦N Má»¤C Lá»¤C CUá»I FILE TRÆ¯á»šC KHI Xá»¬ LÃ ---\n",
    "    # TÃ¬m Ä‘iá»ƒm báº¯t Ä‘áº§u cá»§a Báº£ng tra nhanh Ä‘á»ƒ loáº¡i bá» nÃ³ khá»i pháº§n ná»™i dung chÃ­nh\n",
    "    toc_marker = \"Báº¢NG TRA NHANH CÃC Tá»ˆNH\"\n",
    "    toc_idx = content.rfind(toc_marker)\n",
    "    if toc_idx == -1: toc_idx = content.rfind(\"Báº¢NG TRA NHANH\")\n",
    "    \n",
    "    # Body content chá»‰ láº¥y Ä‘áº¿n trÆ°á»›c má»¥c lá»¥c. \n",
    "    # Pháº§n má»¥c lá»¥c (toc_part) chá»‰ dÃ¹ng Ä‘á»ƒ láº¥y danh sÃ¡ch tÃªn, khÃ´ng dÃ¹ng Ä‘á»ƒ quÃ©t ná»™i dung.\n",
    "    body_content = content[:toc_idx] if toc_idx != -1 else content\n",
    "    toc_part = content[toc_idx:] if toc_idx != -1 else content\n",
    "\n",
    "    # 1. Parse Má»¥c Lá»¥c (Váº«n parse tá»« pháº§n toc_part Ä‘á»ƒ láº¥y danh sÃ¡ch)\n",
    "    provinces, attractions = parse_toc_structure(content) # HÃ m nÃ y giá»¯ nguyÃªn logic cÅ©\n",
    "    if not provinces: return\n",
    "\n",
    "    final_dataset = {\n",
    "        \"General_Information\": extract_general_info(body_content, provinces[0]), \n",
    "        \"Provinces\": []\n",
    "    }\n",
    "\n",
    "    # 2. Cáº¯t ná»™i dung theo tá»«ng Tá»‰nh (DÃ¹ng body_content Ä‘Ã£ sáº¡ch)\n",
    "    print(\"ğŸš€ Äang xá»­ lÃ½ chi tiáº¿t tá»«ng tá»‰nh (ÄÃ£ Fix lá»—i YÃªn BÃ¡i)...\")\n",
    "    \n",
    "    prov_markers = []\n",
    "    for p in provinces:\n",
    "        # Regex tÃ¬m tÃªn tá»‰nh trong body_content\n",
    "        m = re.search(rf'(?:\\n|^)\\s*{re.escape(p)}\\s*(?:\\n|$)', body_content)\n",
    "        if m: prov_markers.append({\"name\": p, \"start\": m.start()})\n",
    "    \n",
    "    prov_markers.sort(key=lambda x: x[\"start\"])\n",
    "\n",
    "    for i, p_mk in enumerate(prov_markers):\n",
    "        p_name = p_mk[\"name\"]\n",
    "        start = p_mk[\"start\"]\n",
    "        # Äiá»ƒm cuá»‘i cá»§a tá»‰nh nÃ y lÃ  Ä‘iá»ƒm báº¯t Ä‘áº§u cá»§a tá»‰nh sau.\n",
    "        # Vá»›i tá»‰nh cuá»‘i cÃ¹ng (YÃªn BÃ¡i), Ä‘iá»ƒm cuá»‘i chÃ­nh lÃ  len(body_content) (Ä‘Ã£ cáº¯t má»¥c lá»¥c)\n",
    "        end = prov_markers[i+1][\"start\"] if i+1 < len(prov_markers) else len(body_content)\n",
    "        \n",
    "        prov_text = body_content[start:end]\n",
    "        \n",
    "        province_obj = {\n",
    "            \"id\": f\"P_{i:03d}\",\n",
    "            \"name\": p_name,\n",
    "            \"items\": []\n",
    "        }\n",
    "\n",
    "        # 3. QuÃ©t Ä‘á»‹a danh trong text cá»§a tá»‰nh\n",
    "        found_attrs = []\n",
    "        for attr in attractions:\n",
    "            # FIX 2: Regex cháº·t cháº½ hÆ¡n Ä‘á»ƒ trÃ¡nh báº¯t nháº§m Header\n",
    "            # Chá»‰ báº¯t náº¿u theo sau khÃ´ng pháº£i lÃ  dÃ²ng chá»‰ chá»©a sá»‘ trang\n",
    "            am = re.search(rf'(?:\\n|^)\\s*(?:\\d+\\.\\s*)?{re.escape(attr)}\\s*(?:\\n|\\.\\.\\.)', prov_text, re.IGNORECASE)\n",
    "            if am:\n",
    "                found_attrs.append({\"name\": attr, \"start\": am.start(), \"end_header\": am.end()})\n",
    "        \n",
    "        found_attrs.sort(key=lambda x: x[\"start\"])\n",
    "        \n",
    "        if not found_attrs:\n",
    "            content_clean = clean_text(prov_text.replace(p_name, \"\"))\n",
    "            # Chá»‰ thÃªm náº¿u ná»™i dung khÃ´ng pháº£i lÃ  header rÃ¡c\n",
    "            if len(content_clean) > 50 and \"THáº®NG Cáº¢NH\" not in content_clean:\n",
    "                province_obj[\"items\"].append({\n",
    "                    \"name\": f\"Tá»•ng quan {p_name}\",\n",
    "                    \"type\": \"General\",\n",
    "                    \"content\": content_clean,\n",
    "                    \"embedding_text\": f\"Tá»•ng quan vá» tá»‰nh {p_name}. {content_clean[:300]}\"\n",
    "                })\n",
    "        else:\n",
    "            # Láº¥y Intro\n",
    "            intro_text = prov_text[:found_attrs[0][\"start\"]].replace(p_name, \"\")\n",
    "            intro_clean = clean_text(intro_text)\n",
    "            if len(intro_clean) > 50:\n",
    "                 province_obj[\"items\"].append({\n",
    "                    \"name\": f\"Giá»›i thiá»‡u chung {p_name}\",\n",
    "                    \"type\": \"General\",\n",
    "                    \"content\": intro_clean,\n",
    "                    \"embedding_text\": f\"Tá»•ng quan vá» tá»‰nh {p_name}. {intro_clean[:300]}\"\n",
    "                })\n",
    "            \n",
    "            # Láº¥y tá»«ng Ä‘á»‹a danh\n",
    "            for k, attr in enumerate(found_attrs):\n",
    "                a_name = attr[\"name\"]\n",
    "                a_start = attr[\"end_header\"]\n",
    "                a_end = found_attrs[k+1][\"start\"] if k+1 < len(found_attrs) else len(prov_text)\n",
    "                \n",
    "                raw_content = prov_text[a_start:a_end]\n",
    "                \n",
    "                # FIX 3: Loáº¡i bá» cÃ¡c dÃ²ng Header thá»«a bá»‹ dÃ­nh vÃ o ná»™i dung\n",
    "                # VÃ­ dá»¥: Loáº¡i bá» dÃ²ng \"THáº®NG Cáº¢NH, DI TÃCH VÃ€ Lá»„ Há»˜I\" náº¿u nÃ³ náº±m trong content\n",
    "                raw_content = re.sub(r'THáº®NG Cáº¢NH, DI TÃCH VÃ€ Lá»„ Há»˜I', '', raw_content)\n",
    "                \n",
    "                a_content = clean_text(raw_content)\n",
    "                \n",
    "                # Chá»‰ láº¥y náº¿u ná»™i dung Ä‘á»§ dÃ i (trÃ¡nh láº¥y nháº§m dÃ²ng tiÃªu Ä‘á»)\n",
    "                if len(a_content) > 30: \n",
    "                    label = \"Place\"\n",
    "                    if any(x in a_name.upper() for x in [\"Lá»„\", \"Há»˜I\"]): label = \"Festival\"\n",
    "                    elif any(x in a_name.upper() for x in [\"CHÃ™A\", \"Äá»€N\", \"ÄÃŒNH\", \"MIáº¾U\", \"NHÃ€ THá»œ\"]): label = \"Religious\"\n",
    "                    elif any(x in a_name.upper() for x in [\"NÃšI\", \"SÃ”NG\", \"Há»’\", \"THÃC\", \"Rá»ªNG\", \"HANG\", \"Äá»˜NG\"]): label = \"Nature\"\n",
    "                    elif any(x in a_name.upper() for x in [\"Báº¢O TÃ€NG\", \"DI TÃCH\"]): label = \"History\"\n",
    "                    \n",
    "                    province_obj[\"items\"].append({\n",
    "                        \"name\": a_name,\n",
    "                        \"type\": label,\n",
    "                        \"content\": a_content,\n",
    "                        \"embedding_text\": f\"{a_name} ({label}) táº¡i {p_name}. {a_content[:300]}\"\n",
    "                    })\n",
    "\n",
    "        final_dataset[\"Provinces\"].append(province_obj)\n",
    "        print(f\"  -> {p_name}: {len(province_obj['items'])} má»¥c.\")\n",
    "\n",
    "    with open(OUTPUT_FILENAME, 'w', encoding='utf-8') as f:\n",
    "        json.dump(final_dataset, f, ensure_ascii=False, indent=2)\n",
    "    \n",
    "    print(f\"\\nâœ… HOÃ€N Táº¤T FIX! File '{OUTPUT_FILENAME}' Ä‘Ã£ sáº¡ch lá»—i YÃªn BÃ¡i.\")\n",
    "    if not os.path.exists(file_path):\n",
    "        print(f\"âŒ File not found: {file_path}\")\n",
    "        return\n",
    "\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        content = f.read()\n",
    "\n",
    "    # 1. Parse Má»¥c Lá»¥c\n",
    "    provinces, attractions = parse_toc_structure(content)\n",
    "    if not provinces: return\n",
    "\n",
    "    final_dataset = {\n",
    "        \"General_Information\": extract_general_info(content, provinces[0]), # Láº¥y pháº§n Ä‘áº§u sÃ¡ch\n",
    "        \"Provinces\": []\n",
    "    }\n",
    "\n",
    "    # 2. Cáº¯t ná»™i dung theo tá»«ng Tá»‰nh\n",
    "    print(\"ğŸš€ Äang xá»­ lÃ½ chi tiáº¿t tá»«ng tá»‰nh...\")\n",
    "    \n",
    "    # TÃ¬m vá»‹ trÃ­ báº¯t Ä‘áº§u cá»§a má»—i tá»‰nh trong text\n",
    "    prov_markers = []\n",
    "    for p in provinces:\n",
    "        # Regex tÃ¬m tÃªn tá»‰nh Ä‘á»©ng riÃªng dÃ²ng hoáº·c in hoa\n",
    "        m = re.search(rf'(?:\\n|^)\\s*{re.escape(p)}\\s*(?:\\n|$)', content)\n",
    "        if m: prov_markers.append({\"name\": p, \"start\": m.start()})\n",
    "    \n",
    "    prov_markers.sort(key=lambda x: x[\"start\"])\n",
    "\n",
    "    for i, p_mk in enumerate(prov_markers):\n",
    "        p_name = p_mk[\"name\"]\n",
    "        start = p_mk[\"start\"]\n",
    "        # Káº¿t thÃºc lÃ  tá»‰nh tiáº¿p theo hoáº·c Ä‘áº¿n pháº§n Má»¥c Lá»¥c\n",
    "        end = prov_markers[i+1][\"start\"] if i+1 < len(prov_markers) else content.rfind(\"Báº¢NG TRA NHANH\")\n",
    "        \n",
    "        prov_text = content[start:end]\n",
    "        \n",
    "        province_obj = {\n",
    "            \"id\": f\"P_{i:03d}\",\n",
    "            \"name\": p_name,\n",
    "            \"items\": []\n",
    "        }\n",
    "\n",
    "        # 3. Trong má»—i tá»‰nh, quÃ©t tÃ¬m cÃ¡c Äá»‹a danh/Lá»… há»™i tá»« danh sÃ¡ch Attractions\n",
    "        found_attrs = []\n",
    "        for attr in attractions:\n",
    "            # TÃ¬m Ä‘á»‹a danh trong text cá»§a tá»‰nh nÃ y\n",
    "            # Cho phÃ©p sá»‘ thá»© tá»±: \"1. ChÃ¹a HÆ°Æ¡ng\"\n",
    "            am = re.search(rf'(?:\\n|^)\\s*(?:\\d+\\.\\s*)?{re.escape(attr)}\\s*(?:\\n|\\.\\.\\.)', prov_text, re.IGNORECASE)\n",
    "            if am:\n",
    "                found_attrs.append({\"name\": attr, \"start\": am.start(), \"end_header\": am.end()})\n",
    "        \n",
    "        found_attrs.sort(key=lambda x: x[\"start\"])\n",
    "        \n",
    "        # 4. Cáº¯t text cho tá»«ng Ä‘á»‹a danh tÃ¬m tháº¥y\n",
    "        if not found_attrs:\n",
    "            # Náº¿u khÃ´ng tÃ¬m tháº¥y Ä‘á»‹a danh nÃ o, láº¥y toÃ n bá»™ text lÃ m \"Giá»›i thiá»‡u chung\"\n",
    "            province_obj[\"items\"].append({\n",
    "                \"name\": f\"Tá»•ng quan {p_name}\",\n",
    "                \"type\": \"General\",\n",
    "                \"content\": clean_text(prov_text.replace(p_name, \"\"))\n",
    "            })\n",
    "        else:\n",
    "            # Láº¥y pháº§n Ä‘áº§u (Intro)\n",
    "            intro_text = prov_text[:found_attrs[0][\"start\"]].replace(p_name, \"\")\n",
    "            if len(intro_text.strip()) > 50:\n",
    "                province_obj[\"items\"].append({\n",
    "                    \"name\": f\"Giá»›i thiá»‡u chung {p_name}\",\n",
    "                    \"type\": \"General\",\n",
    "                    \"content\": clean_text(intro_text),\n",
    "                    \"embedding_text\": f\"Tá»•ng quan vá» tá»‰nh {p_name}. {intro_text[:300]}\"\n",
    "                })\n",
    "            \n",
    "            # Láº¥y tá»«ng Ä‘á»‹a danh\n",
    "            for k, attr in enumerate(found_attrs):\n",
    "                a_name = attr[\"name\"]\n",
    "                a_start = attr[\"end_header\"]\n",
    "                a_end = found_attrs[k+1][\"start\"] if k+1 < len(found_attrs) else len(prov_text)\n",
    "                \n",
    "                a_content = clean_text(prov_text[a_start:a_end])\n",
    "                \n",
    "                if len(a_content) > 20: # Bá» qua rÃ¡c\n",
    "                    # Auto-Labeling Ä‘Æ¡n giáº£n\n",
    "                    label = \"Place\"\n",
    "                    if any(x in a_name.upper() for x in [\"Lá»„\", \"Há»˜I\"]): label = \"Festival\"\n",
    "                    elif any(x in a_name.upper() for x in [\"CHÃ™A\", \"Äá»€N\", \"ÄÃŒNH\", \"MIáº¾U\"]): label = \"Religious\"\n",
    "                    elif any(x in a_name.upper() for x in [\"NÃšI\", \"SÃ”NG\", \"Há»’\", \"THÃC\", \"Rá»ªNG\"]): label = \"Nature\"\n",
    "                    \n",
    "                    province_obj[\"items\"].append({\n",
    "                        \"name\": a_name,\n",
    "                        \"type\": label,\n",
    "                        \"content\": a_content,\n",
    "                        \"embedding_text\": f\"{a_name} ({label}) táº¡i {p_name}. {a_content[:300]}\"\n",
    "                    })\n",
    "\n",
    "        final_dataset[\"Provinces\"].append(province_obj)\n",
    "        print(f\"  -> {p_name}: {len(province_obj['items'])} má»¥c.\")\n",
    "\n",
    "    # Xuáº¥t file JSON\n",
    "    with open(OUTPUT_FILENAME, 'w', encoding='utf-8') as f:\n",
    "        json.dump(final_dataset, f, ensure_ascii=False, indent=2)\n",
    "    \n",
    "    print(f\"\\nğŸ‰ HOÃ€N Táº¤T! ÄÃ£ xuáº¥t toÃ n bá»™ dá»¯ liá»‡u ra '{OUTPUT_FILENAME}'\")\n",
    "    \n",
    "    # Thá»‘ng kÃª\n",
    "    total_items = sum(len(p['items']) for p in final_dataset['Provinces']) + len(final_dataset['General_Information'])\n",
    "    print(f\"ğŸ“Š Tá»•ng cá»™ng: {len(final_dataset['Provinces'])} Tá»‰nh vÃ  {total_items} Node dá»¯ liá»‡u.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    extract_all_content(INPUT_FILENAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dd7b387",
   "metadata": {},
   "source": [
    "### => Váº«n tá»“n Ä‘á»ng lá»—i trÃªn, chÆ°a triá»‡t Ä‘á»ƒ\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "498e9ccf",
   "metadata": {},
   "source": [
    "### Tiáº¿p tá»¥c fix lá»—i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bca2815c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” Äang phÃ¢n tÃ­ch Má»¥c Lá»¥c...\n",
      "ğŸ“– Äang láº¥y pháº§n Äáº¡i CÆ°Æ¡ng...\n",
      "ğŸš€ Äang xá»­ lÃ½ chi tiáº¿t (ÄÃ£ fix lá»—i YÃªn BÃ¡i & Header)...\n",
      "  -> HÃ€ Ná»˜I: 178 má»¥c.\n",
      "  -> AN GIANG: 17 má»¥c.\n",
      "  -> BÃ€ Rá»ŠA-VÅ¨NG TÃ€U: 33 má»¥c.\n",
      "  -> Báº C LIÃŠU: 12 má»¥c.\n",
      "  -> Báº®C GIANG: 26 má»¥c.\n",
      "  -> Báº®C Káº N: 22 má»¥c.\n",
      "  -> Báº®C NINH: 19 má»¥c.\n",
      "  -> Báº¾N TRE: 19 má»¥c.\n",
      "  -> BÃŒNH DÆ¯Æ NG: 15 má»¥c.\n",
      "  -> BÃŒNH Äá»ŠNH: 24 má»¥c.\n",
      "  -> BÃŒNH PHÆ¯á»šC: 13 má»¥c.\n",
      "  -> BÃŒNH THUáº¬N: 32 má»¥c.\n",
      "  -> CÃ€ MAU: 16 má»¥c.\n",
      "  -> CAO Báº°NG: 30 má»¥c.\n",
      "  -> THÃ€NH PHá» Cáº¦N THÆ : 14 má»¥c.\n",
      "  -> THÃ€NH PHá» ÄÃ€ Náº´NG: 21 má»¥c.\n",
      "  -> Äáº®K Láº®K: 14 má»¥c.\n",
      "  -> Äáº®K NÃ”NG: 16 má»¥c.\n",
      "  -> ÄIá»†N BIÃŠN: 11 má»¥c.\n",
      "  -> Äá»’NG NAI: 18 má»¥c.\n",
      "  -> Äá»’NG THÃP: 18 má»¥c.\n",
      "  -> GIA LAI: 18 má»¥c.\n",
      "  -> HÃ€ GIANG: 25 má»¥c.\n",
      "  -> HÃ€ NAM: 54 má»¥c.\n",
      "  -> HÃ€ TÃ‚Y: 25 má»¥c.\n",
      "  -> HÃ€ TÄ¨NH: 23 má»¥c.\n",
      "  -> Háº¢I DÆ¯Æ NG: 22 má»¥c.\n",
      "  -> Háº¢I PHÃ’NG: 19 má»¥c.\n",
      "  -> Háº¬U GIANG: 7 má»¥c.\n",
      "  -> HÃ’A BÃŒNH: 15 má»¥c.\n",
      "  -> THÃ€NH PHá» Há»’ CHÃ MINH: 48 má»¥c.\n",
      "  -> HÆ¯NG YÃŠN: 38 má»¥c.\n",
      "  -> KIÃŠN GIANG: 27 má»¥c.\n",
      "  -> KON TUM: 13 má»¥c.\n",
      "  -> LAI CHÃ‚U: 6 má»¥c.\n",
      "  -> Láº NG SÆ N: 29 má»¥c.\n",
      "  -> LÃ€O CAI: 23 má»¥c.\n",
      "  -> LÃ‚M Äá»’NG: 37 má»¥c.\n",
      "  -> LONG AN: 13 má»¥c.\n",
      "  -> NAM Äá»ŠNH: 22 má»¥c.\n",
      "  -> NGHá»† AN: 23 má»¥c.\n",
      "  -> NINH BÃŒNH: 21 má»¥c.\n",
      "  -> NINH THUáº¬N: 15 má»¥c.\n",
      "  -> PHÃš THá»Œ: 15 má»¥c.\n",
      "  -> PHÃš YÃŠN: 14 má»¥c.\n",
      "  -> QUáº¢NG BÃŒNH: 20 má»¥c.\n",
      "  -> QUáº¢NG NAM: 28 má»¥c.\n",
      "  -> QUáº¢NG NGÃƒI: 21 má»¥c.\n",
      "  -> QUáº¢NG NINH: 32 má»¥c.\n",
      "  -> QUáº¢NG TRá»Š: 16 má»¥c.\n",
      "  -> SÃ“C TRÄ‚NG: 17 má»¥c.\n",
      "  -> SÆ N LA: 13 má»¥c.\n",
      "  -> TÃ‚Y NINH: 10 má»¥c.\n",
      "  -> THÃI BÃŒNH: 21 má»¥c.\n",
      "  -> THÃI NGUYÃŠN: 17 má»¥c.\n",
      "  -> THANH HÃ“A: 34 má»¥c.\n",
      "  -> THá»ªA THIÃŠN-HUáº¾: 28 má»¥c.\n",
      "  -> TIá»€N GIANG: 21 má»¥c.\n",
      "  -> TRÃ€ VINH: 14 má»¥c.\n",
      "  -> TUYÃŠN QUANG: 18 má»¥c.\n",
      "  -> VÄ¨NH LONG: 14 má»¥c.\n",
      "  -> VÄ¨NH PHÃšC: 14 má»¥c.\n",
      "  -> YÃŠN BÃI: 14 má»¥c.\n",
      "\n",
      "âœ… ÄÃƒ XONG! File '../data/VanHoaVaDuLichVN.json' Ä‘Ã£ sáº¡ch 100%.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import json\n",
    "import os\n",
    "\n",
    "# TÃªn file cá»§a báº¡n\n",
    "INPUT_FILENAME = '../data/VanHoaVaDuLichVN.txt'\n",
    "OUTPUT_FILENAME = '../data/VanHoaVaDuLichVN.json'\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"LÃ m sáº¡ch vÄƒn báº£n OCR\"\"\"\n",
    "    if not text: return \"\"\n",
    "    text = re.sub(r'\\n\\s*\\d+\\s*\\n', '\\n', text) # XÃ³a sá»‘ trang láº» loi\n",
    "    text = re.sub(r'\\.{4,}\\d*', '', text) # XÃ³a cháº¥m má»¥c lá»¥c\n",
    "    # Ná»‘i dÃ²ng bá»‹ ngáº¯t\n",
    "    lines = text.split('\\n')\n",
    "    cleaned_lines = []\n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "        if not line: continue\n",
    "        # Náº¿u dÃ²ng trÆ°á»›c khÃ´ng káº¿t thÃºc báº±ng dáº¥u cÃ¢u vÃ  dÃ²ng nÃ y khÃ´ng viáº¿t hoa -> Ná»‘i\n",
    "        if cleaned_lines and not cleaned_lines[-1].endswith(('.', ':', '!', '?')) and not line[0].isupper():\n",
    "            cleaned_lines[-1] += \" \" + line\n",
    "        else:\n",
    "            cleaned_lines.append(line)\n",
    "    return \"\\n\".join(cleaned_lines)\n",
    "\n",
    "def parse_toc_structure(content):\n",
    "    \"\"\"Láº¥y danh sÃ¡ch Tá»‰nh vÃ  Äá»‹a danh tá»« Má»¥c Lá»¥c\"\"\"\n",
    "    print(\"ğŸ” Äang phÃ¢n tÃ­ch Má»¥c Lá»¥c...\")\n",
    "    \n",
    "    # TÃ¬m vÃ¹ng Má»¥c lá»¥c á»Ÿ cuá»‘i sÃ¡ch\n",
    "    start_marker = \"Báº¢NG TRA NHANH CÃC Tá»ˆNH\"\n",
    "    start_idx = content.rfind(start_marker)\n",
    "    if start_idx == -1: start_idx = content.rfind(\"Báº¢NG TRA NHANH\")\n",
    "    \n",
    "    if start_idx == -1:\n",
    "        print(\"âŒ Lá»—i: KhÃ´ng tÃ¬m tháº¥y Má»¥c Lá»¥c. HÃ£y kiá»ƒm tra láº¡i file text.\")\n",
    "        return [], []\n",
    "\n",
    "    toc_content = content[start_idx:]\n",
    "    \n",
    "    # TÃ¡ch pháº§n Tá»‰nh vÃ  pháº§n Tháº¯ng cáº£nh\n",
    "    parts = toc_content.split(\"Báº¢NG TRA CÃC THáº®NG Cáº¢NH\")\n",
    "    toc_provinces_text = parts[0]\n",
    "    toc_attractions_text = parts[1] if len(parts) > 1 else \"\"\n",
    "\n",
    "    line_pattern = re.compile(r'^(.+?)(?:\\.{3,})\\s*\\d+$', re.MULTILINE)\n",
    "\n",
    "    provinces = [m.strip() for m in line_pattern.findall(toc_provinces_text) if len(m.strip()) > 3 and \"Báº¢NG TRA\" not in m]\n",
    "    attractions = [m.strip() for m in line_pattern.findall(toc_attractions_text) if len(m.strip()) > 3]\n",
    "\n",
    "    # Sáº¯p xáº¿p Ä‘á»™ dÃ i giáº£m dáº§n Ä‘á»ƒ khá»›p chÃ­nh xÃ¡c\n",
    "    provinces = sorted(list(set(provinces)), key=len, reverse=True)\n",
    "    attractions = sorted(list(set(attractions)), key=len, reverse=True)\n",
    "    \n",
    "    return provinces, attractions\n",
    "\n",
    "def extract_general_info(content, first_province):\n",
    "    \"\"\"Láº¥y pháº§n Äáº¡i CÆ°Æ¡ng Ä‘áº§u sÃ¡ch\"\"\"\n",
    "    print(\"ğŸ“– Äang láº¥y pháº§n Äáº¡i CÆ°Æ¡ng...\")\n",
    "    match = re.search(rf'\\n\\s*{re.escape(first_province)}\\s*\\n', content)\n",
    "    end_idx = match.start() if match else 0\n",
    "    general_text = content[:end_idx]\n",
    "    \n",
    "    chapters = [\"LÃƒNH THá»” VIá»†T NAM\", \"Lá»ŠCH Sá»¬ VIá»†T NAM\", \"CÃC DÃ‚N Tá»˜C TRÃŠN Äáº¤T NÆ¯á»šC VIá»†T NAM\",\n",
    "                \"TÃ”N GIÃO - TÃN NGÆ¯á» NG\", \"PHONG Tá»¤C Táº¬P QUÃN\", \"Lá»„ Há»˜I TRUYá»€N THá»NG\", \"Ä‚N Máº¶C Cá»¦A NGÆ¯á»œI VIá»†T\"]\n",
    "    \n",
    "    data = []\n",
    "    indices = []\n",
    "    for chap in chapters:\n",
    "        m = re.search(rf'(?:\\n|^)\\s*{re.escape(chap)}', general_text)\n",
    "        if m: indices.append({\"name\": chap, \"start\": m.start()})\n",
    "    indices.sort(key=lambda x: x[\"start\"])\n",
    "    \n",
    "    for i, item in enumerate(indices):\n",
    "        name = item[\"name\"]\n",
    "        start = item[\"start\"]\n",
    "        end = indices[i+1][\"start\"] if i+1 < len(indices) else len(general_text)\n",
    "        content_chap = clean_text(general_text[start:end])\n",
    "        data.append({\n",
    "            \"id\": f\"GEN_{i:02d}\", \"name\": name, \"type\": \"GeneralTopic\",\n",
    "            \"content\": content_chap,\n",
    "            \"embedding_text\": f\"Tá»•ng quan VÄƒn hÃ³a Viá»‡t Nam - {name}: {content_chap[:300]}\"\n",
    "        })\n",
    "    return data\n",
    "\n",
    "def extract_all_content_fixed(file_path):\n",
    "    if not os.path.exists(file_path):\n",
    "        print(\"âŒ KhÃ´ng tÃ¬m tháº¥y file.\")\n",
    "        return\n",
    "\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        full_content = f.read()\n",
    "\n",
    "    # --- QUAN TRá»ŒNG: Cáº®T Bá» Má»¤C Lá»¤C KHá»I Ná»˜I DUNG ---\n",
    "    toc_marker = \"Báº¢NG TRA NHANH CÃC Tá»ˆNH\"\n",
    "    toc_idx = full_content.rfind(toc_marker)\n",
    "    if toc_idx == -1: toc_idx = full_content.rfind(\"Báº¢NG TRA NHANH\")\n",
    "    \n",
    "    # Body content chá»‰ láº¥y Ä‘áº¿n trÆ°á»›c má»¥c lá»¥c -> Kháº¯c phá»¥c lá»—i YÃªn BÃ¡i bá»‹ láº«n\n",
    "    body_content = full_content[:toc_idx] if toc_idx != -1 else full_content\n",
    "\n",
    "    # 1. Láº¥y danh sÃ¡ch tá»« Má»¥c Lá»¥c (váº«n dÃ¹ng full_content Ä‘á»ƒ quÃ©t danh sÃ¡ch)\n",
    "    provinces, attractions = parse_toc_structure(full_content)\n",
    "    if not provinces: return\n",
    "\n",
    "    final_dataset = {\n",
    "        \"General_Information\": extract_general_info(body_content, provinces[0]),\n",
    "        \"Provinces\": []\n",
    "    }\n",
    "\n",
    "    # 2. Xá»­ lÃ½ tá»«ng Tá»‰nh\n",
    "    print(\"ğŸš€ Äang xá»­ lÃ½ chi tiáº¿t (ÄÃ£ fix lá»—i YÃªn BÃ¡i & Header)...\")\n",
    "    \n",
    "    prov_markers = []\n",
    "    for p in provinces:\n",
    "        m = re.search(rf'(?:\\n|^)\\s*{re.escape(p)}\\s*(?:\\n|$)', body_content)\n",
    "        if m: prov_markers.append({\"name\": p, \"start\": m.start()})\n",
    "    prov_markers.sort(key=lambda x: x[\"start\"])\n",
    "\n",
    "    for i, p_mk in enumerate(prov_markers):\n",
    "        p_name = p_mk[\"name\"]\n",
    "        start = p_mk[\"start\"]\n",
    "        # Äiá»ƒm cuá»‘i lÃ  tá»‰nh tiáº¿p theo hoáº·c Háº¾T body_content\n",
    "        end = prov_markers[i+1][\"start\"] if i+1 < len(prov_markers) else len(body_content)\n",
    "        \n",
    "        prov_text = body_content[start:end]\n",
    "        \n",
    "        province_obj = { \"id\": f\"P_{i:03d}\", \"name\": p_name, \"items\": [] }\n",
    "\n",
    "        # TÃ¬m Ä‘á»‹a danh trong tá»‰nh nÃ y\n",
    "        found_attrs = []\n",
    "        for attr in attractions:\n",
    "            am = re.search(rf'(?:\\n|^)\\s*(?:\\d+\\.\\s*)?{re.escape(attr)}\\s*(?:\\n|\\.\\.\\.)', prov_text, re.IGNORECASE)\n",
    "            if am: found_attrs.append({\"name\": attr, \"start\": am.start(), \"end_header\": am.end()})\n",
    "        found_attrs.sort(key=lambda x: x[\"start\"])\n",
    "        \n",
    "        # Náº¿u khÃ´ng cÃ³ Ä‘á»‹a danh -> Láº¥y háº¿t lÃ m Tá»•ng quan\n",
    "        if not found_attrs:\n",
    "            clean_c = clean_text(prov_text.replace(p_name, \"\"))\n",
    "            # Lá»c rÃ¡c header\n",
    "            clean_c = re.sub(r'THáº®NG Cáº¢NH, DI TÃCH VÃ€ Lá»„ Há»˜I', '', clean_c).strip()\n",
    "            \n",
    "            if len(clean_c) > 50:\n",
    "                province_obj[\"items\"].append({\n",
    "                    \"name\": f\"Tá»•ng quan {p_name}\", \"type\": \"General\",\n",
    "                    \"content\": clean_c,\n",
    "                    \"embedding_text\": f\"Tá»•ng quan vá» tá»‰nh {p_name}. {clean_c[:300]}\"\n",
    "                })\n",
    "        else:\n",
    "            # Láº¥y Intro\n",
    "            intro_raw = prov_text[:found_attrs[0][\"start\"]].replace(p_name, \"\")\n",
    "            intro_clean = clean_text(re.sub(r'THáº®NG Cáº¢NH, DI TÃCH VÃ€ Lá»„ Há»˜I', '', intro_raw))\n",
    "            if len(intro_clean) > 50:\n",
    "                 province_obj[\"items\"].append({\n",
    "                    \"name\": f\"Giá»›i thiá»‡u chung {p_name}\", \"type\": \"General\",\n",
    "                    \"content\": intro_clean,\n",
    "                    \"embedding_text\": f\"Tá»•ng quan vá» tá»‰nh {p_name}. {intro_clean[:300]}\"\n",
    "                })\n",
    "            \n",
    "            # Láº¥y tá»«ng Ä‘á»‹a danh\n",
    "            for k, attr in enumerate(found_attrs):\n",
    "                a_name = attr[\"name\"]\n",
    "                a_start = attr[\"end_header\"]\n",
    "                a_end = found_attrs[k+1][\"start\"] if k+1 < len(found_attrs) else len(prov_text)\n",
    "                \n",
    "                raw_content = prov_text[a_start:a_end]\n",
    "                # --- QUAN TRá»ŒNG: XÃ“A HEADER RÃC ---\n",
    "                raw_content = re.sub(r'THáº®NG Cáº¢NH, DI TÃCH VÃ€ Lá»„ Há»˜I', '', raw_content)\n",
    "                a_content = clean_text(raw_content)\n",
    "                \n",
    "                # Chá»‰ láº¥y náº¿u ná»™i dung thá»±c sá»± dÃ i (trÃ¡nh láº¥y nháº§m tiÃªu Ä‘á» rá»—ng)\n",
    "                if len(a_content) > 40: \n",
    "                    label = \"Place\"\n",
    "                    if any(x in a_name.upper() for x in [\"Lá»„\", \"Há»˜I\"]): label = \"Festival\"\n",
    "                    elif any(x in a_name.upper() for x in [\"CHÃ™A\", \"Äá»€N\", \"ÄÃŒNH\", \"MIáº¾U\", \"NHÃ€ THá»œ\"]): label = \"Religious\"\n",
    "                    elif any(x in a_name.upper() for x in [\"NÃšI\", \"SÃ”NG\", \"Há»’\", \"THÃC\", \"Rá»ªNG\", \"HANG\", \"Äá»˜NG\"]): label = \"Nature\"\n",
    "                    \n",
    "                    province_obj[\"items\"].append({\n",
    "                        \"name\": a_name, \"type\": label,\n",
    "                        \"content\": a_content,\n",
    "                        \"embedding_text\": f\"{a_name} ({label}) táº¡i {p_name}. {a_content[:300]}\"\n",
    "                    })\n",
    "\n",
    "        final_dataset[\"Provinces\"].append(province_obj)\n",
    "        print(f\"  -> {p_name}: {len(province_obj['items'])} má»¥c.\")\n",
    "\n",
    "    with open(OUTPUT_FILENAME, 'w', encoding='utf-8') as f:\n",
    "        json.dump(final_dataset, f, ensure_ascii=False, indent=2)\n",
    "    \n",
    "    print(f\"\\nâœ… ÄÃƒ XONG! File '{OUTPUT_FILENAME}' Ä‘Ã£ sáº¡ch 100%.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    extract_all_content_fixed(INPUT_FILENAME)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
